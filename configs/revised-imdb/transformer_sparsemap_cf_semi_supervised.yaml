# general args
seed: 1    # for reproducibility
default_root_dir: 'experiments/cfrat-revised-imdb-sparsemap-cf-semi-supervised-transformer/'  # path to save logs and models


# training args
train:
    # wandb
    wandb_project: 'cfrat'
    wandb_entity: 'mtreviso'

    # io
    save_rationales: True       # will be saved in a .txt in the default_root_dir as `{test|val}_rationales.txt`
    save_counterfactuals: True  # will be saved in a .txt in the default_root_dir as `{test|val}_counterfactuals.txt`
    save_tokenizer: False       # the tokenizer will be pickled and store in the checkpoint dir as `tokenizer.pickle`
    save_label_encoder: False   # the label encoder will be pickled and store in the checkpoint dir as `label_encoder.pickle`
    gpu_log: False              # whether to use the gpu callback to see gpu information in the logger

    # data
    dm: 'revised_imdb_cf'           # data module name (see docs for more options)
    batch_size: 8                   # minibatch size
    num_workers: 1                  # number of workers used for data loading (0 means that only a single core will be used)
    vocab_min_occurrences:  1       # frequency for a token to be added to the vocabulary
    max_seq_len: 512                # pretrained transformer limitation
    # max_dataset_size: 64          # for quick testing
    is_original: null               # filter out samples by originality: True, False, or None (no filter, default)

    # early stopping
    monitor: 'val_cf_accuracy'     # quantity to be monitored
    monitor_mode: 'max'         # whether to see if monitored metric has stopped decreasing (min) or increasing (max)
    monitor_patience: 5         # number of epochs to wait for early stopping

    # pytorch-lightning rationalizer model
    model: 'transformer_spectra_cf_semi_supervised'

    # factual flow
    # ckpt: null
    factual_ckpt: experiments/cfrat-revised-imdb-sparsemap-full-transformer/version174wy4rb/checkpoints/epoch=8.ckpt
    cf_gen_ckpt: experiments/cfrat-editor-t5/version1qwb1xtb/checkpoints/epoch=9.ckpt
    load_tokenizer: False
    load_label_encoder: False

    tokenizer: 't5-small'
    gen_arch: 't5-small'

    gen_emb_requires_grad: False
    gen_encoder_requires_grad: False
    gen_use_decoder: False
    pred_emb_requires_grad: False
    pred_arch: 't5-small'
    pred_bidirectional: False
    pred_encoder_requires_grad: False
    pred_output_requires_grad: False
    shared_gen_pred: False

    explainer: 'sparsemap'
    explainer_pre_mlp: True
    explainer_requires_grad: True
    sparsemap_budget: 30
    sparsemap_temperature: 0.01
    sparsemap_transition: 0.0001

    dropout: 0.1
    selection_space: 'embedding'
    selection_vector: 'zero'
    selection_faithfulness: True
    selection_mask: False
    temperature: 1.0

    # supervision
    cf_prepend_label_type: 'gold'
    cf_z_type: 'pred'
    cf_task_name: "binary_classification"
    cf_explainer_mask_token_type_id: null
    cf_lbda_gen: 0.00
    cf_use_reinforce: True
    cf_use_reinforce_baseline: True

    # counterfactual flow
    cf_tokenizer: 't5-small'
    cf_gen_arch: "t5-small"

    cf_gen_emb_requires_grad: False
    cf_gen_encoder_requires_grad: False
    cf_gen_lm_head_requires_grad: False

    cf_dropout: 0.1
    cf_input_space: 'embedding'
    cf_selection_vector: 'zero'
    cf_selection_mask: False
    cf_selection_faithfulness: True
    cf_selection_mode: 'none'

    cf_generate_kwargs:
        do_sample: False            # deterministic or stochastic generation
        num_beams: 1                # mice: 15
        num_beam_groups: 1          # to promote diversity in beam groups
        early_stopping: False       # stop the beam search when at least `num_beams` sentences are finished
        length_penalty: 1.0         # (<1) means shorter sequences, (>1) longer sequences
        top_k: 10                   # default: 50  | mice: 30
        top_p: 1.0                  # default: 1.0 | mice: 0.95
        typical_p: null             # default: 1.0
        no_repeat_ngram_size: 0     # no bigrams repetitions are allowed (default: 0 | mice: 2)
        num_return_sequences: 1     # sample N sequences
        max_length: 512             # maximum length of the generated sequence (default: 512 | mice: 512)

    # both flows
    share_generators: False      # share generators between flows

    # model: optimizer
    optimizer: 'adamw'
    lr: 0.00001
    cf_lr: 0.0001
    ff_lbda: 0.1
    cf_lbda: 1.0

    weight_decay: 0.000001
    betas: [0.9, 0.999]
    amsgrad: False
    momentum: 0.0
    dampening: 0.0
    nesterov: False
    alpha: 0.99   # for rmsprop
    centered: False  # for rmsprop
    lambd: 0.0001  # for asgd
    t0: 1000000.0  # for asgd

    # model: lr scheduler
    #scheduler: 'multistep'
    #milestones: [25, 50, 75]
    #lr_decay: 0.97  # a.k.a gamma

    # trainer (will be passed to pytorch-lightning's Trainer object)
    # see the complete list here: https://pytorch-lightning.readthedocs.io/en/stable/trainer.html#trainer-flags
    accelerator: gpu
    devices: 1
    gradient_clip_val: 5.0
    min_epochs: 3
    max_epochs: 10
    #limit_test_batches:
    #limit_train_batches: 10
    #limit_val_batches: 1
    #log_every_n_steps: 25


# the options defined here will overwrite the ones defined in the checkpoint
predict:
    # ckpt: null               # will be defined via cli --ckpt or will get last checkpoint version if it exists
    accelerator: gpu
    devices: 1
    load_tokenizer: False       # load a trained tokenizer stored in the checkpoint dir as `tokenizer.pickle`
    load_label_encoder: False   # load a trained label encoder stored in the checkpoint dir as `label_encoder.pickle`
